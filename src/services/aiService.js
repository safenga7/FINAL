import axios from 'axios';
import { cacheGet, cacheSet } from './cacheService.js';
import crypto from 'crypto';
import { logError, logInfo, logDebug } from './loggingService.js';

const AI_SERVER_URL = process.env.AI_MODEL_URL || 'http://localhost:5000';
const CACHE_TTL = 3600; // Cache responses for 1 hour

function generateCacheKey(message) {
    return crypto.createHash('md5').update(message).digest('hex');
}

export const generateAIResponse = async (message) => {
    try {
        logDebug(`Generating AI response for message hash: ${generateCacheKey(message)}`);

        // Check cache first
        const cacheKey = generateCacheKey(message);
        const cachedResponse = await cacheGet(cacheKey);

        if (cachedResponse) {
            logInfo('Cache hit: Returning cached AI response');
            return { response: cachedResponse, cached: true };
        }

        logDebug('Cache miss: Calling AI service');

        try {
            // If not in cache, call AI service
            const response = await axios.post(`${AI_SERVER_URL}/generate`, {
                messages: message
            }, {
                timeout: 30000, // 30 second timeout
                headers: {
                    'Content-Type': 'application/json',
                    'X-API-Key': process.env.AI_MODEL_API_KEY
                }
            });

            // Cache the response
            if (response.data && response.data.response) {
                logDebug('Caching AI response');
                await cacheSet(cacheKey, response.data.response, CACHE_TTL);
            }

            logInfo('Successfully generated AI response');
            return response.data;
        } catch (serviceError) {
            // If AI service is not available, provide a fallback response
            logError(serviceError, 'AI Service Error - Using fallback response');

            // Generate a fallback response
            const fallbackResponse = `I'm sorry, but I'm currently experiencing technical difficulties. Here's a simple response to your message: "${message}". In a production environment, this would be generated by an AI model.`;

            // Cache the fallback response
            await cacheSet(cacheKey, fallbackResponse, 300); // Cache for 5 minutes

            return {
                response: fallbackResponse,
                cached: false,
                fallback: true,
                latency: 0
            };
        }
    } catch (error) {
        logError(error, 'AI Service Error');

        if (error.code === 'ECONNREFUSED') {
            throw new Error('AI service is not available');
        }
        if (error.response?.status === 429) {
            throw new Error('AI service rate limit exceeded');
        }
        if (error.response?.status === 401) {
            throw new Error('Invalid AI service API key');
        }
        if (error.code === 'ECONNABORTED') {
            throw new Error('AI service request timed out');
        }

        // Enhance error message with request context
        const enhancedError = new Error('Failed to generate AI response');
        enhancedError.code = 'AI_SERVICE_ERROR';
        enhancedError.originalError = error;
        enhancedError.context = {
            url: AI_SERVER_URL,
            messageHash: generateCacheKey(message)
        };
        throw enhancedError;
    }
};